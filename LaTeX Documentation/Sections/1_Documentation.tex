\setcounter{page}{1}
\section{Portfolio documentation}
\label{sec:documentation}

Compile a comprehensive documentation of your project, including all the project phases. You will need to explain every choice you made during the project and your thoughts about the results you get. You will introduce the results in suitable visualisation. Furthermore, you will need to explain which criteria you follow to build your prompts and how they affect the results. 

Students write the entire documentation with sections, sub-sections, diagrams, etc in this section. Please write as comprehensively as possible. Head to the document 1\_documentation.tex. You are free to use as many subsections as required. We will not provide a template for documentation. 

\subsection{Research Phase}
\label{subsec:research}
In the research phase, there are two selections I'm going to make: the dataset selection and the model selection.

\subsubsection{[Dataset A] Selection}
With regards to the dataset of this language translation task,
the basic requirement is that it should come with at least 1,000 pairs of German-French translations.
So the selected dataset should include german and french languages, at least 1,000 rows and preferably be used for translation tasks.
I applied these 3 filters to huggingface datasets and it gave me 88 datasets. 
I went through some of them and tried to get to know
1)how the data format looks like; 
2)what the dataset originally is used for;
3)how the translation quality is. 

First, the dataset must be a translation between German and French.
The google/wmt24pp dataset is a translation between German and English, and French and English,
so I excluded this type of data and focused on finding a dataset of translation between German and French.
This can filter out many datasets, and finally 
I focused on the Helsinki-NLP/opus-100\cite{zhang-etal-2020-improving}\cite{tiedemann-2012-parallel} dataset, 
the Helsinki-NLP/opus\_books dataset, and the Helsinki-NLP/europarl\cite{koehn-2005-europarl} dataset.

Second, this task does not limit translations to specific fields.
The Helsinki-NLP/opus-100 dataset is an English-centered multilingual corpus.
All training pairs contain English on the source or target side,
and the official does not specify a specific field.
The Helsinki-NLP/opus\_books dataset is taken from
a collection of copyright free books aligned by Andras Farkas.
The Helsinki-NLP/europarl dataset comes from the European Parliament, 
and the content mainly involves law and politics. 
Because the task is not limited, it makes me a little entangled.
So, I decided to refer to the third point and randomly check the quality of the data.

Third, after comparison,
I found that the Helsinki-NLP/opus\_books dataset contains strange semantic translations, 
such as one data "de": "Den ich wandern muß, arms Waisenkind! Weshalb sandten sie mich so weit, so weit," 
"fr": "«Pourquoi m'ont-ils envoyé si seul et si loin, là où s'étendent les marécages, 
là où sont amoncelés les sombres rochers?" 
Even if I used Google Translate, I couldn't understand it, 
which would inevitably affect fine-tuning, 
and the dataset is biased towards the storyline, 
which I don't think is suitable for this fine-tuning, 
so I excluded the Helsinki-NLP/opus\_books dataset.
The data in Helsinki-NLP/opus-100 also has a similar situation. 
One of the sentences is "de": "Schweröle, ausgenommen Schmieröle für 
Uhrmacherei und dergleichen in kleinen Behältern mit einem Inhalt von 
bis zu 250 Gramm Öl netto", "fr": "27101931 à 27101999" 
This does not match at all, and most of them are short sentences, 
and the quality is not high. The data in Helsinki-NLP/europarl are mostly long sentences. 
In addition, the content of the Helsinki-NLP/europarl dataset focuses on parliament and has higher quality. 
So I finally decided to use Helsinki-NLP/europarl as my [Dataset A].

\subsubsection{[Model A] Selection}
As for Model A, 
the most basic requirement is a small pre-trained model 
that can be fine-tuned within a free Google Colab notebook 
(T4 GPU, 15 GB VRAM, 12.7 GB RAM). 
In addition, I also need to consider that this task is German-French translation, 
Model A needs to support multiple languages, 
and it is best to have a good community ecosystem and related documentation.

In the practice class, 
I saw the teaching assistant used Llama related documents, 
which aroused my curiosity. 
So I went to learn about Llama and found that Llama 3.2 1B is a good choice. 
According to the official website \cite{llama-website}, 
the 1B model requires at least 4GB VRAM, and may be more during training. 
It is suitable for inference and lightweight fine-tuning on Colab.
3B may require more than 16GB VRAM during training, 
and requires additional optimization when training on Colab, and it may not be enough. 
So I decided to use Llama 3.2 1B to try the generated translation effect first.

\subsection{Design Phase}
\label{subsec:design}

\subsubsection{Fine-tuning Approach}
Considering
(1) the limited computing environment on Google Colab (T4 GPU, 15GB VRAM, 12.7GB RAM), 
(2) Llama-3.2-1B has 1 billion parameters, and 
(3) Dataset A has only 1,000 pairs of German-French translations,
LoRA (Low-Rank Adaptation) \cite{hu2021lora} or its improved version 
QLoRA (Quantized LoRA) \cite{dettmers2023qlora} could be an efficient fine-tuning method.

LoRA inserts a small number of trainable parameters into the model 
through low-rank decomposition without changing the parameters of the original model, 
significantly reducing the training parameters and saving VRAM. 
However, the complete model weights still need to be loaded into the VRAM.

QLoRA quantizes the model weights (like 4-bit quantization) based on LoRA, 
thereby significantly reducing VRAM usage.
Although quantization reduces the precision of floating-point numbers, 
it will not have much impact on the German-French translation task, 
and it can also speed up the training process.

Also, the PEFT (Parameter-Efficient Fine-Tuning) library \cite{peft} from huggingface provides these fine-tuning methods.
Through the interface of the PEFT library, 
I can easily integrate the QLoRA method into my training process and automatically quantize and fine-tune the parameters.
Therefore I decided to use QLoRA from the PERT library as my fine-tuning method.

\subsubsection{Split Ratio within [Dataset A]}
First, because a larger model will be used in the subsequent step to 
generate a synthetic data set to optimize the model using a cross-validation strategy, 
as required by the task, a validation set is not required.

Second, because I randomly selected 1,000 pairs of German-French translation data from Dataset A, 
and then divided these 1,000 pairs of German-French translations into training and test datasets. 
When the amount of data is small, a larger training set 
(such as 80\% training set and 20\% test set or 90\% training set and 10\% test set) 
is usually selected to enhance the learning ability of the model.

Finally, because the test set will be used to evaluate different versions of the model, 
a slightly larger test set of 20\% will be more stable than a 10\% test set, 
and the volatility of the evaluation results will be smaller.

So I chose to split [Dataset A] into 80\% training set and 20\% test set.

\subsubsection{Prompt for Querying A Large Model}

\subsubsection{Evaluation Metric}
BLEU (Bilingual Evaluation Understudy)\cite{10.3115/1073083.1073135} is the most commonly used 
machine translation evaluation indicator and is suitable for most translation tasks. 
BLEU is suitable for evaluating the accuracy of short text translation, 
such as short sentences and vocabulary-level translation. 
Considering that this translation task is sentence-level translation and 
most of the test set are short sentences, I chose BLEU as the evaluation metric.

\subsection{Implementation Phase}
\label{subsec:implementation}

https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id