\setcounter{page}{1}
\section{Portfolio documentation}
\label{sec:documentation}

Compile a comprehensive documentation of your project, including all the project phases. You will need to explain every choice you made during the project and your thoughts about the results you get. You will introduce the results in suitable visualisation. Furthermore, you will need to explain which criteria you follow to build your prompts and how they affect the results. 

Students write the entire documentation with sections, sub-sections, diagrams, etc in this section. Please write as comprehensively as possible. Head to the document 1\_documentation.tex. You are free to use as many subsections as required. We will not provide a template for documentation. 

\subsection{Research Phase}
\label{subsec:research}
In the research phase, there are two selections I'm going to make: the dataset selection and the model selection.

\subsubsection{[Dataset A] Selection}
With regards to the dataset of this language translation task,
the basic requirement is that it should come with at least 1,000 pairs of German-French translations.
So the selected dataset should include german and french languages, at least 1,000 rows and preferably be used for translation tasks.
I applied these 3 filters to huggingface datasets and it gave me 88 datasets. 
I went through some of them and tried to get to know
1)how the data format looks like; 
2)what the dataset originally is used for;
3)how the translation quality is. 

First, the dataset must be a translation between German and French.
The google/wmt24pp dataset is a translation between German and English, and French and English,
so I excluded this type of data and focused on finding a dataset of translation between German and French.
This can filter out many datasets, and finally 
I focused on the Helsinki-NLP/opus-100\cite{zhang-etal-2020-improving}\cite{tiedemann-2012-parallel} dataset, 
the Helsinki-NLP/opus\_books dataset, and the Helsinki-NLP/europarl\cite{koehn-2005-europarl} dataset.

Second, this task does not limit translations to specific fields.
The Helsinki-NLP/opus-100 dataset is an English-centered multilingual corpus.
All training pairs contain English on the source or target side,
and the official does not specify a specific field.
The Helsinki-NLP/opus\_books dataset is taken from
a collection of copyright free books aligned by Andras Farkas.
The Helsinki-NLP/europarl dataset comes from the European Parliament, 
and the content mainly involves law and politics. 
Because the task is not limited, it makes me a little entangled.
So, I decided to refer to the third point and randomly check the quality of the data.

Third, after comparison,
I found that the Helsinki-NLP/opus\_books dataset contains strange semantic translations, 
such as one data "de": "Den ich wandern muß, arms Waisenkind! Weshalb sandten sie mich so weit, so weit," 
"fr": "«Pourquoi m'ont-ils envoyé si seul et si loin, là où s'étendent les marécages, 
là où sont amoncelés les sombres rochers?" 
Even if I used Google Translate, I couldn't understand it, 
which would inevitably affect fine-tuning, 
and the dataset is biased towards the storyline, 
which I don't think is suitable for this fine-tuning, 
so I excluded the Helsinki-NLP/opus\_books dataset.
The data in Helsinki-NLP/opus-100 also has a similar situation. 
One of the sentences is "de": "Schweröle, ausgenommen Schmieröle für 
Uhrmacherei und dergleichen in kleinen Behältern mit einem Inhalt von 
bis zu 250 Gramm Öl netto", "fr": "27101931 à 27101999" 
This does not match at all, and most of them are short sentences, 
and the quality is not high. The data in Helsinki-NLP/europarl are mostly long sentences. 
In addition, the content of the Helsinki-NLP/europarl dataset focuses on parliament and has higher quality. 
So I finally decided to use Helsinki-NLP/europarl as my [Dataset A].

\subsubsection{[Model A] Selection}
As for Model A, 
the most basic requirement is a small pre-trained model 
that can be fine-tuned within a free Google Colab notebook 
(T4 GPU, 15 GB VRAM, 12.7 GB RAM). 
In addition, I also need to consider that this task is German-French translation, 
Model A needs to support multiple languages, 
and it is best to have a good community ecosystem and related documentation.


I heard about Llama in the practice class, 
and I went to learn about Llama and found that Llama 3.2 1B is a good choice. 
According to the official website \cite{llama-website}, 
the 1B model requires at least 4GB VRAM, and may be more during training. 
It is suitable for inference and lightweight fine-tuning on Colab.
3B may require more than 16GB VRAM during training, 
and requires additional optimization when training on Colab, and it may not be enough. 
So I decided to use Llama 3.2 1B to try the generated translation effect.
In addition, I specifically chose the Instruction-tuned version of llama 
so that the model can better understand from the instructions that the task type is translation.

\subsection{Design Phase}
\label{subsec:design}

\subsubsection{Fine-tuning Approach}
Considering
(1) the limited computing environment on Google Colab (T4 GPU, 15GB VRAM, 12.7GB RAM), 
(2) Llama-3.2-1B has 1 billion parameters, and 
(3) Dataset A has only 1,000 pairs of German-French translations,
LoRA (Low-Rank Adaptation) \cite{hu2021lora} could be an efficient fine-tuning method.

LoRA inserts a small number of trainable parameters into the model 
through low-rank decomposition without changing the parameters of the original model, 
significantly reducing the training parameters and saving VRAM. 
However, the complete model weights still need to be loaded into the VRAM.

Also, the PEFT (Parameter-Efficient Fine-Tuning) library \cite{peft} from huggingface provides these fine-tuning methods.
Through the interface of the PEFT library, 
I can easily integrate the LoRA method into my training process and automatically quantize and fine-tune the parameters.
Therefore I decided to use LoRA from the PERT library as my fine-tuning method.

\subsubsection{Split Ratio within [Dataset A]}
First, because a larger model will be used in the subsequent step to 
generate a synthetic data set to optimize the model using a cross-validation strategy, 
as required by the task, a validation set is not required.

Second, because I randomly selected 1,000 pairs of German-French translation data from Dataset A, 
and then divided these 1,000 pairs of German-French translations into training and test datasets. 
When the amount of data is small, a larger training set 
(such as 80\% training set and 20\% test set or 90\% training set and 10\% test set) 
is usually selected to enhance the learning ability of the model.

Finally, because the test set will be used to evaluate different versions of the model, 
a slightly larger test set of 20\% will be more stable than a 10\% test set, 
and the volatility of the evaluation results will be smaller.

So I chose to split [Dataset A] into 80\% training set and 20\% test set.

\subsubsection{Prompt for Querying A Large Model}
First, I chose DeepSeek-V3\cite{DeepSeek-V3}, which has more than 32B parameters. 
The reasons are: 
(1) DeepSeek-V3 has no less than 32B parameters; 
(2) DeepSeek-V3 supports German and French; 
(3) DeepSeek-V3 training covers the text of the European Parliament; 
(4) DeepSeek-V3's API is cheaper than OpenAI's.

Second, when querying the large model to generate 1,600 pairs of German-French data sets, 
the prompts are divided into system prompts and user prompts. 
The system prompt is 

"""You are a professional DE-FR corpus generator. Create batch translations with:
    1. German: 20-30 words, formal EU parliamentary style
    2. French: Accurate technical translations
    3. Format:
    [{"de": "German text", "fr": "French translation"},...]""",

and the user prompt is

f"""Generate a batch of {batch\_size} German-French translation pairs.
Requirements:
- Unique legislative contexts per pair
- No repeated phrases
- Valid JSON array format
- No markdown formatting""".

Of course, I have used deepseek itself to help me optimize prompts. 
This is something I learned when doing the DIFY exercise, and the prompt can be further strengthened through large language models.
And it is not difficult to understand from the prompts themselves.
First, the two prompts are divided into system-level context prompts and user-level task prompts according to the principle of role separation.
Secondly, the system-level context prompt tells the big model that its role is a professional German-French bilingual translation generator. 
It also tells it that the topic of the dataset to be generated is the German-French translation pairs related to the European Parliament, 
each German sentence has 20-30 words, and requires an accurate and professional French translation. 
Finally, the data format is standardized to facilitate subsequent processing into a data structure.
Third, the user task-level prompt tells the model how many pairs of translations are needed for a single batch, 
and they must be non-repetitive and preferably have different legislative backgrounds. 
It also emphasizes that the format should be JSON, not markdown, to facilitate subsequent data processing.
Eventually, the final result was indeed in line with expectations, and [Dataset B] was synthesized smoothly.

\subsubsection{Evaluation Metric}
BLEU (Bilingual Evaluation Understudy)\cite{10.3115/1073083.1073135} is the most commonly used 
machine translation evaluation indicator and is suitable for most translation tasks. 
BLEU is suitable for evaluating the accuracy of short text translation, 
such as short sentences and vocabulary-level translation. 
Considering that this translation task is sentence-level translation and 
most of the test set are short sentences, I chose BLEU as the evaluation metric.

\subsection{Implementation Phase}
\label{subsec:implementation}
The implementation phase is divided into 12 steps according to the guidance of the task book.

1. Load [Dataset A] and split it according to the designed ratio.
Because of the GenAI course, I started to progress from hearing about huggingface to actually using it. 
Not only are the datasets and large models found on huggingface open source, 
but the platform also provides many ready-made tool libraries needed for large models.
The first step is to use the huggingface dataset library to load the europarl dataset, 
and randomly select 1000 pairs according to the fixed random seed value 123 to ensure the repeatability of the experiment. 
Then divide the training set and test set according to the 8:2 ratio planned in the design stage. 
However, I didn't quite understand why there was no validation set to assist in training at first, 
but I understood it later because a larger dataset will be generated to strengthen the data comparison.

2. Load your chosen pre-trained model [Model A].
The second step is to use the pipeline method provided by the huggingface library to load the model and the corresponding tokenizer. 
The pipeline highly abstracts the loading of large models and can make reasoning more convenient.
But in fact, I struggled for a long time on this step and the choice of model. 
I even spent almost two days on deciding whether to use the ordinary version of llama or the instruction-tuned version of llama. 
Finally, by comparing and running the evaluation of the third step several times, 
I found that the instruction-tuned llama can indeed understand and obey instructions better than the ordinary version of llama, 
and the instruction-tuned version of llama also supports pipeline messages, 
while the ordinary version of llama does not support it, which makes me a little confused.
Unfortunately, I don't have time to study it in detail.

3. Evaluate [Model A] on the test dataset [Dataset A: Test] using the chosen metric.

4. Fine-tune [Model A] on the training dataset [Dataset A: Train] to create [Model B].

5. Evaluate [Model B] on the test dataset [Dataset A: Test] using the chosen metric.

6. Use the designed prompt to generate a new synthesized dataset [Dataset B],
twice the size of the training set [Dataset A: Train], using the selected larger model.

7. Fine-tune [Model A] on the synthesized dataset [Dataset B] to create [Model C].

8. Evaluate [Model C] on the test dataset [Dataset A: Test] using the chosen metric.

9. Combine [Dataset A: Train] and [Dataset B], shuffle them with suitable seeds, and create [Dataset C].

10. Fine-tune [Model A] on the combined dataset [Dataset C] to create [Model D].

11. Evaluate [Model D] on the test dataset [Dataset A: Test] using the chosen metric.

12. Plot the performance of all models using appropriate visualizations.

